<!DOCTYPE html>
<html lang=" en-US ">

<head>
	<meta charset='utf-8'>
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" href="/assets/css/style.css?v=78c8719698feee0553096165409216db0a43f06f">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.css">

	<style>
		.navbar {
			overflow: hidden;
		}

		.navbar a {
			float: left;
			font-size: 16px;
			color: white;
			padding: 2px 15px;
			text-align: center;
			text-decoration: none;
		}

		.dropdown {
			float: left;
			overflow: hidden;
		}

		.dropdown .dropbtn {
			font-size: 16px;
			border: none;
			outline: none;
			color: white;
			padding: 2px 15px;
			background-color: inherit;
			font-family: inherit;
			margin: 0;
		}

		.navbar a:hover,
		.dropdown:hover .dropbtn {
			background-color: green;
		}

		.dropdown-content {
			display: none;
			position: absolute;
			background-color: #202020;
			min-width: 150px;
			box-shadow: 0px 8px 16px 0px rgba(0, 0, 0, 0.2);
			z-index: 1;
		}

		.dropdown-content a {
			float: none;
			color: white;
			padding: 8px 14px;
			text-decoration: none;
			display: block;
			text-align: left;
		}

		.dropdown-content a:hover {
			background-color: #4666FF;
		}

		.dropdown:hover .dropdown-content {
			display: block;
		}
	</style>

	<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Machine Learning with sckit-learn | datdhruvjain.github.io</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Machine Learning with sckit-learn" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Interesting learnings on Computer stuff." />
<meta property="og:description" content="Interesting learnings on Computer stuff." />
<link rel="canonical" href="http://192.168.1.117:4000/prog_langs/python/Data-Science/Machine_learning_scikit_learn.html" />
<meta property="og:url" content="http://192.168.1.117:4000/prog_langs/python/Data-Science/Machine_learning_scikit_learn.html" />
<meta property="og:site_name" content="datdhruvjain.github.io" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Machine Learning with sckit-learn" />
<script type="application/ld+json">
{"@type":"WebPage","url":"http://192.168.1.117:4000/prog_langs/python/Data-Science/Machine_learning_scikit_learn.html","headline":"Machine Learning with sckit-learn","description":"Interesting learnings on Computer stuff.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

<body>

	<header>
		<div class="container">
			<a id="a-title" href="/">
				<h1>datdhruvjain.github.io</h1>
			</a>
			<h2>Interesting learnings on Computer stuff.</h2>

			<!--- need to check what this is all about
		<section id="downloads">
		
		</section>
		--->

			<section class="navbar">


				<!-- THE NON-DROPDOWN MENUS WORK LIKE THIS
				----------------------------------------
				
				<a href="https://datdhruvjain.github.io/directory/file">Title 1</a>
				<a href="./relative-file">Title 2</a>
				<a href="../dir/relative-file-path">Title 3</a>
				<a href="https://datdhruvjain.github.io/blockchain/blockchain">Blockchain</a>

				#	Do Not add add the extention to the file, eg do not put file.md, just file
				---------------------------------------->

				<!-- THE DROPDOWN MENUS WORK LIKE THIS
				----------------------------------------
				
				<div class="dropdown">
					<button class="dropbtn">dropdown-title
						<i class="fa fa-caret-down"></i>
					</button>
					<div class="dropdown-content">
						<a href="https://datdhruvjain.github.io/directory/file">Title 1</a>
						<a href="./relative-file">Title 2</a>
						<a href="../dir/relative-file-path">Title 3</a>
					</div>
				</div>

				#	Do Not add add the extention to the file, eg do not put file.md, just file
				------------------------------------------>

				<a href="/configurations/configurations">My Configs</a>


				<div class="dropdown">
					<button class="dropbtn">Journeys
						<i class="fa fa-caret-down"></i>
					</button>
					<div class="dropdown-content">
						<!--a href="https://datdhruvjain.github.io/journeys/blockchain/blockchain">Blockchain</a-->
						<a href="/journeys/cybersec/cybersec">Cyber Sec</a>
						<a href="/journeys/thm/thm">TryHackMe Journeys</a>
					</div>
				</div>

				<div class="dropdown">
					<button class="dropbtn">Prog Langs
						<i class="fa fa-caret-down"></i>
					</button>
					<div class="dropdown-content">
						<a href="/prog_langs/python/python">Python</a>
						<a href="/prog_langs/cpp/cpp">C++</a>
					</div>
				</div>

				<div class="dropdown">
					<button class="dropbtn">Tools
						<i class="fa fa-caret-down"></i>
					</button>
					<div class="dropdown-content">
						<a href="/tools/git">Git</a>
						<a href="/tools/vim/vim-learnings">Vim</a>
						<a href="/tools/markdown">Markdown</a>
						<a href="/tools/computer-networks/computer-networks">Computer-Networks</a>
					</div>
				</div>


				<!-- add new items from here -->
			</section>

		</div>
	</header>

	<div class="container">
		<section id="main_content">
			<h1 id="machine-learning-with-sckit-learn">Machine Learning with sckit-learn</h1>

<h2 id="1-import-pandas-and-load-the-dataset">1. import pandas and load the dataset</h2>

<p>pandas is a library made for statistical computations. It is based off of numpy</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="s">'path/to/file.csv'</span>
<span class="n">my_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
</code></pre></div></div>

<p>You can inspect the data using head(), describe() and column.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_data</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>    <span class="c1"># Give the First 5 data for each columns
</span><span class="n">my_data</span><span class="p">.</span><span class="n">describe</span><span class="p">()</span> <span class="c1"># Show the whole data set
</span><span class="n">my_data</span><span class="p">.</span><span class="n">columns</span>    <span class="c1"># show the tilte for each columns
</span></code></pre></div></div>

<h2 id="2-select-the-prediction-target">2. Select the prediction target</h2>

<p>the prediction target is <strong>what you want to predict</strong></p>

<p>‘y’ is conventionally used for this</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">my_data</span><span class="p">.</span><span class="n">Prediction_target</span>
</code></pre></div></div>

<h2 id="3-choosing-the-features">3. Choosing the “Features”</h2>

<p>The features are aspects of the data which influence the prediction.</p>

<p>For eg if you are predicting House Price, the locality, size of house, floor it is on, will all affect/influence it’s price.</p>

<p>It is generally stored in a list, and are strings which are headings of the columns.</p>

<p>Further the <strong>dataset</strong> is stored conventionally in ‘X’ as follows</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Features</span> <span class="o">=</span> <span class="p">[</span><span class="s">'feature1'</span><span class="p">,</span> <span class="s">'feature2'</span><span class="p">,</span> <span class="s">'feature3'</span><span class="p">,</span> <span class="s">'feature4'</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">my_data</span><span class="p">[</span><span class="n">Features</span><span class="p">]</span> 
</code></pre></div></div>

<h2 id="4-building-your-model">4. Building Your Model</h2>

<p>The steps to building and using a model are:</p>

<ul>
  <li><strong>Define:</strong> What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.</li>
  <li><strong>Fit:</strong> Capture patterns from provided data. This is the heart of modeling.</li>
  <li><strong>Predict:</strong> Just what it sounds like</li>
  <li><strong>Evaluate</strong>: Determine how accurate the model’s predictions are.</li>
</ul>

<p>Here is an example of defining a decision tree model with scikit-learn and fitting it with the features and target variable.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>

<span class="c1"># Define model. Specify a number for random_state to ensure same results each run
</span><span class="n">melbourne_model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Fit model
</span><span class="n">melbourne_model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1"># X is the "Training input samples"
# y is the target values (real numbers)
</span></code></pre></div></div>

<p>Many machine learning models allow some randomness in model training. Specifying a number for <code class="language-plaintext highlighter-rouge">random_state</code> ensures you get the same results in each run. This is considered a good practice. You use any number, and model quality won’t depend meaningfully on exactly what value you choose.</p>

<p>We now have a fitted model that we can use to make predictions.</p>

<p>We can Check the predictions for the <strong>First 5 datum as follows</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Making predictions for the following 5 houses:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">head</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The predictions are"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">melbourne_model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">head</span><span class="p">()))</span>
<span class="c1"># The "fit" method takes X as a parameter which is the input samples
# It returns an array of predicted classes
</span></code></pre></div></div>

<h2 id="validating-with-mean-avg-error">Validating with Mean Avg Error</h2>

<p>Higher the error, worse the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>

<span class="n">predicted_home_prices</span> <span class="o">=</span> <span class="n">melbourne_model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predicted_home_prices</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="test-train-split">Test-Train Split</h2>

<p>The scikit-learn library has a function <code class="language-plaintext highlighter-rouge">train_test_split</code> to break up the data into two pieces. We’ll use some of that data as training data to fit the model, and we’ll use the other data as validation data to calculate <code class="language-plaintext highlighter-rouge">mean_absolute_error</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># split data into training and validation data, for both features and target
# The split is based on a random number generator. Supplying a numeric value to
# the random_state argument guarantees we get the same split every time we
# run this script.
</span><span class="n">train_X</span><span class="p">,</span> <span class="n">val_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="c1"># Define model
</span><span class="n">melbourne_model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">()</span>
<span class="c1"># Fit model
</span><span class="n">melbourne_model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>

<span class="c1"># get predicted prices on validation data
</span><span class="n">val_predictions</span> <span class="o">=</span> <span class="n">melbourne_model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">val_X</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">val_y</span><span class="p">,</span> <span class="n">val_predictions</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="models-can-suffer-from-either">Models can suffer from either:</h3>

<ul>
  <li><strong>Overfitting:</strong> capturing spurious patterns that won’t recur in the future, leading to less accurate predictions, or</li>
  <li><strong>Underfitting:</strong> failing to capture relevant patterns, again leading to less accurate predictions.</li>
</ul>

<p>We use <strong>validation</strong> data, which isn’t used in model training, to measure a candidate model’s accuracy. This lets us try many candidate models and keep the best one.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_mae</span><span class="p">(</span><span class="n">max_leaf_nodes</span><span class="p">,</span> <span class="n">train_X</span><span class="p">,</span> <span class="n">val_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">max_leaf_nodes</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
    <span class="n">preds_val</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">val_X</span><span class="p">)</span>
    <span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">val_y</span><span class="p">,</span> <span class="n">preds_val</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">mae</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">DecisionTreeRegressor()</code> takes an optional value called <code class="language-plaintext highlighter-rouge">max_leaf_nodes</code> which specifies the number of nodes or ‘leafs’ that the model should have</li>
</ul>

<h2 id="random-forests">Random Forests</h2>

<p>Decision trees leave you with a difficult decision. A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data.</p>

<p>Even today’s most sophisticated modeling techniques face this tension between underfitting and overfitting. But, many models have clever ideas that can lead to better performance. We’ll look at the <strong>random forest</strong> as an example.</p>

<p>The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. If you keep modeling, you can learn more models with even better performance, but many of those are sensitive to getting the right parameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>

<span class="n">forest_model</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">forest_model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
<span class="n">melb_preds</span> <span class="o">=</span> <span class="n">forest_model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">val_X</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">val_y</span><span class="p">,</span> <span class="n">melb_preds</span><span class="p">))</span>
</code></pre></div></div>

<h2 id="another-example">Another Example</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="s">"Survived"</span><span class="p">]</span>

<span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="s">"Pclass"</span><span class="p">,</span> <span class="s">"Sex"</span><span class="p">,</span> <span class="s">"SibSp"</span><span class="p">,</span> <span class="s">"Parch"</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="n">features</span><span class="p">])</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="n">features</span><span class="p">])</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'PassengerId'</span><span class="p">:</span> <span class="n">test_data</span><span class="p">.</span><span class="n">PassengerId</span><span class="p">,</span> <span class="s">'Survived'</span><span class="p">:</span> <span class="n">predictions</span><span class="p">})</span>
</code></pre></div></div>

<h2 id="dealing-with-missing-values">Dealing with Missing Values</h2>

<ul>
  <li>
    <h3 id="dropping-missing-values">Dropping missing Values</h3>

    <p>One way to deal with missing data is to drop the tables with missing values, for eg</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cols_with_missing</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">X_train</span><span class="p">.</span><span class="n">columns</span>
                     <span class="k">if</span> <span class="n">X_train</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">isnull</span><span class="p">().</span><span class="nb">any</span><span class="p">()]</span>
  
<span class="c1"># Drop columns in training and validation data
</span><span class="n">reduced_X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">cols_with_missing</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">reduced_X_valid</span> <span class="o">=</span> <span class="n">X_valid</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">cols_with_missing</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <h3 id="using-imputation">Using Imputation</h3>

    <p>We can use imputation to deal with <code class="language-plaintext highlighter-rouge">NULL</code> or <code class="language-plaintext highlighter-rouge">NaN</code> values and is done as follows:</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>

<span class="c1"># Imputation
</span><span class="n">my_imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">()</span>
<span class="n">imputed_X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">my_imputer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
<span class="n">imputed_X_valid</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">my_imputer</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_valid</span><span class="p">))</span>

<span class="c1"># Imputation removed column names; put them back
</span><span class="n">imputed_X_train</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">columns</span>
<span class="n">imputed_X_valid</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">X_valid</span><span class="p">.</span><span class="n">columns</span>

<span class="k">print</span><span class="p">(</span><span class="s">"MAE from Approach 2 (Imputation):"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">score_dataset</span><span class="p">(</span><span class="n">imputed_X_train</span><span class="p">,</span> <span class="n">imputed_X_valid</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">))</span>
</code></pre></div></div>

<p>In this method we fill in the missing values with mean values. Other values can also be inserted but the performance difference usually negligible.</p>

<ul>
  <li>
    <h3 id="imputation-part-deux">Imputation Part Deux</h3>

    <p>Another way is to impute as well as record what values were imputed. This is done by adding another column to the dataset.</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Make copy to avoid changing original data (when imputing)
</span><span class="n">X_train_plus</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">X_valid_plus</span> <span class="o">=</span> <span class="n">X_valid</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Make new columns indicating what will be imputed
</span><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cols_with_missing</span><span class="p">:</span>
    <span class="n">X_train_plus</span><span class="p">[</span><span class="n">col</span> <span class="o">+</span> <span class="s">'_was_missing'</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_train_plus</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">isnull</span><span class="p">()</span>
    <span class="n">X_valid_plus</span><span class="p">[</span><span class="n">col</span> <span class="o">+</span> <span class="s">'_was_missing'</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_valid_plus</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">isnull</span><span class="p">()</span>

<span class="c1"># Imputation
</span><span class="n">my_imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">()</span>
<span class="n">imputed_X_train_plus</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">my_imputer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_plus</span><span class="p">))</span>
<span class="n">imputed_X_valid_plus</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">my_imputer</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_valid_plus</span><span class="p">))</span>

<span class="c1"># Imputation removed column names; put them back
</span><span class="n">imputed_X_train_plus</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">X_train_plus</span><span class="p">.</span><span class="n">columns</span>
<span class="n">imputed_X_valid_plus</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">X_valid_plus</span><span class="p">.</span><span class="n">columns</span>

<span class="k">print</span><span class="p">(</span><span class="s">"MAE from Approach 3 (An Extension to Imputation):"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">score_dataset</span><span class="p">(</span><span class="n">imputed_X_train_plus</span><span class="p">,</span> <span class="n">imputed_X_valid_plus</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">))</span>
</code></pre></div></div>

<h2 id="categorical-variables">Categorical Variables</h2>

<p>we have three main approaches for categorical Data</p>

<ul>
  <li>
    <h3 id="drop-categorical-variables">Drop Categorical Variables</h3>

    <p>The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information.</p>

    <p>eg</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Get list of categorical variables
</span><span class="n">s</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span><span class="p">.</span><span class="n">dtypes</span> <span class="o">==</span> <span class="s">'object'</span><span class="p">)</span>
<span class="n">object_cols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">s</span><span class="p">].</span><span class="n">index</span><span class="p">)</span>
  
<span class="k">print</span><span class="p">(</span><span class="s">"Categorical variables:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">object_cols</span><span class="p">)</span>
</code></pre></div>    </div>

    <p>We drop the <code class="language-plaintext highlighter-rouge">object</code> columns with the <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html"><code class="language-plaintext highlighter-rouge">select_dtypes()</code></a> method.</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">drop_X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="s">'object'</span><span class="p">])</span>
<span class="n">drop_X_valid</span> <span class="o">=</span> <span class="n">X_valid</span><span class="p">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="s">'object'</span><span class="p">])</span>
  
<span class="k">print</span><span class="p">(</span><span class="s">"MAE from Approach 1 (Drop categorical variables):"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">score_dataset</span><span class="p">(</span><span class="n">drop_X_train</span><span class="p">,</span> <span class="n">drop_X_valid</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">))</span>
</code></pre></div>    </div>
  </li>
  <li>
    <h3 id="label-encoding">Label Encoding</h3>

    <p><strong>Label encoding</strong> assigns each unique value to a different integer.</p>

    <p>Not all categorical variables have a clear ordering in the values, but we refer to those that do as <strong>ordinal variables</strong>. For tree-based models (like decision trees and random forests), you can expect label encoding to work well with ordinal variables.</p>

    <p>This assumption makes sense in this example, because there is an indisputable ranking to the categories. Not all categorical variables have a clear ordering in the values, but we refer to those that do as <strong>ordinal variables</strong>. For tree-based models (like decision trees and random forests), you can expect label encoding to work well with ordinal variables.</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
  
<span class="c1"># Make copy to avoid changing original data 
</span><span class="n">label_X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">label_X_valid</span> <span class="o">=</span> <span class="n">X_valid</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
  
<span class="c1"># Apply label encoder to each column with categorical data
</span><span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">object_cols</span><span class="p">:</span>
    <span class="n">label_X_train</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">col</span><span class="p">])</span>
    <span class="n">label_X_valid</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_valid</span><span class="p">[</span><span class="n">col</span><span class="p">])</span>
  
<span class="k">print</span><span class="p">(</span><span class="s">"MAE from Approach 2 (Label Encoding):"</span><span class="p">)</span> 
<span class="k">print</span><span class="p">(</span><span class="n">score_dataset</span><span class="p">(</span><span class="n">label_X_train</span><span class="p">,</span> <span class="n">label_X_valid</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">))</span>
</code></pre></div>    </div>

    <p><strong>One-hot encoding</strong> creates new columns indicating the presence (or absence) of each possible value in the original data</p>

    <p>In the original dataset, “Color” is a categorical variable with three categories: “Red”, “Yellow”, and “Green”. The corresponding one-hot encoding contains one column for each possible value, and one row for each row in the original dataset. Wherever the original value was “Red”, we put a 1 in the “Red” column; if the original value was “Yellow”, we put a 1 in the “Yellow” column, and so on.</p>

    <p>In contrast to label encoding, one-hot encoding <em>does not</em> assume an ordering of the categories. Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data (e.g., “Red” is neither <em>more</em> nor <em>less</em> than “Yellow”). We refer to categorical variables without an intrinsic ranking as <strong>nominal variables</strong>.</p>

    <p>One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won’t use it for variables taking more than 15 different values).</p>

    <p>We use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"><code class="language-plaintext highlighter-rouge">OneHotEncoder</code></a> class from scikit-learn to get one-hot encodings. There are a number of parameters that can be used to customize its behavior.</p>

    <ul>
      <li>We set <code class="language-plaintext highlighter-rouge">handle_unknown='ignore'</code> to avoid errors when the validation data contains classes that aren’t represented in the training data, and</li>
      <li>setting <code class="language-plaintext highlighter-rouge">sparse=False</code> ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix).</li>
    </ul>

    <p>To use the encoder, we supply only the categorical columns that we want to be one-hot encoded. For instance, to encode the training data, we supply <code class="language-plaintext highlighter-rouge">X_train[object_cols]</code>. (<code class="language-plaintext highlighter-rouge">object_cols</code> in the code cell below is a list of the column names with categorical data, and so <code class="language-plaintext highlighter-rouge">X_train[object_cols]</code> contains all of the categorical data in the training set.)</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
  
<span class="c1"># Apply one-hot encoder to each column with categorical data
</span><span class="n">OH_encoder</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span><span class="o">=</span><span class="s">'ignore'</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">OH_cols_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">OH_encoder</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">object_cols</span><span class="p">]))</span>
<span class="n">OH_cols_valid</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">OH_encoder</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_valid</span><span class="p">[</span><span class="n">object_cols</span><span class="p">]))</span>
  
<span class="c1"># One-hot encoding removed index; put it back
</span><span class="n">OH_cols_train</span><span class="p">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">index</span>
<span class="n">OH_cols_valid</span><span class="p">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">X_valid</span><span class="p">.</span><span class="n">index</span>
  
<span class="c1"># Remove categorical columns (will replace with one-hot encoding)
</span><span class="n">num_X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">object_cols</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">num_X_valid</span> <span class="o">=</span> <span class="n">X_valid</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">object_cols</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  
<span class="c1"># Add one-hot encoded columns to numerical features
</span><span class="n">OH_X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">num_X_train</span><span class="p">,</span> <span class="n">OH_cols_train</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">OH_X_valid</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">num_X_valid</span><span class="p">,</span> <span class="n">OH_cols_valid</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  
<span class="k">print</span><span class="p">(</span><span class="s">"MAE from Approach 3 (One-Hot Encoding):"</span><span class="p">)</span> 
<span class="k">print</span><span class="p">(</span><span class="n">score_dataset</span><span class="p">(</span><span class="n">OH_X_train</span><span class="p">,</span> <span class="n">OH_X_valid</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">))</span>
</code></pre></div>    </div>
  </li>
</ul>

		</section>
	</div>
</body>

</html>